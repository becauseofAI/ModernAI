# <p align="center">Awesome Modern Artificial Intelligence</p>  
<div align="center"><img src="https://emtemp.gcom.cloud/ngw/globalassets/en/articles/images/hype-cycle-for-artificial-intelligence-2023.png"/></div>  

## Large Model Evolutionary Graph
<details>
<summary>LLM</summary>
<div align="center"><img src="https://github.com/Mooler0410/LLMsPracticalGuide/blob/main/imgs/tree.jpg"/></div>  
</details>
<details>
<summary>MLLM (LLaMA-based)</summary>
<div align="center"><img src="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/llama-0628-final.png"/></div>  
</details>

## Large Language Model (LLM)

## Vision Foundation Model (VFM)

## Multimodal Large Language Model (MLLM) / Large Multimodal Model (LMM) 

## Modern Autonomous Driving (MAD)
### End-to-End Solution
1. UniAD: Planning-oriented Autonomous Driving [CVPR2023] [[paper]](https://arxiv.org/pdf/2212.10156.pdf) [[code]](https://github.com/OpenDriveLab/UniAD)
2. Scene as Occupancy [arXiv2306] [[paper]](https://arxiv.org/pdf/2306.02851.pdf) [[code]](https://github.com/OpenDriveLab/OccNet)
3. FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving [arXiv2308] [[paper]](https://arxiv.org/pdf/2308.01006.pdf) [[code]](https://github.com/westlake-autolab/FusionAD)
### with Large Language Model
1. Drive Like a Human: Rethinking Autonomous Driving with Large Language Models [arXiv2307] [[paper]](https://arxiv.org/pdf/2307.07162.pdf) [[code]](https://github.com/PJLab-ADG/DriveLikeAHuman)

## Embodied AI (EAI)

## World Model

## Artificial General Intelligence (AGI)

## SOTA Downstream Task

## New Dataset
